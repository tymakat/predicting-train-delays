{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bffc495-1866-46e6-adf2-fe8c07a29e22",
   "metadata": {},
   "source": [
    "Introduction\n",
    "This notebook is the first step towards creating an algorithm that predicts the probability of train cancellation. Here I am going to analyze the open source dataset about train services that I found, performing data preprocessing and data cleaning based on constraints and requirements that I am going to describe below\n",
    "\n",
    "Constraints\n",
    "The features that would be used to train the model have to follow at least one of the following criteria:\n",
    "a) The information stored in the feature is a part of the user's input\n",
    "Example: The user's input is a city of destination, a city of departure and departure time for their future trip.\n",
    "b) It is possible to retrieve or calculate the information stored in the feature based on the user's input. \n",
    "Example: The user (normally) doesn't know about maintenance works, but it is possible to search for any maintenance on the specified route, using the time and the stations that the user provided\n",
    "\n",
    "(Functional) Requirements\n",
    "\n",
    "The model has to give an estimate on total delay that the user might face, including arrival delay\n",
    "The model has to give an estimate on the train's cancellation \n",
    "The estimation should be given in a form of a percentage\n",
    "The model has to predict the delay probability for every 5-minute batch (example: 10% chance of 5 minute delay, 15% chance of 10 minute delay)\n",
    "The model must be able to solve a multi-label classification problem, as there would be multiple features in y_test\n",
    "\n",
    "At this point you should have all of the neccessary information to understand the logic behind my future steps. Let's start the data preprocessing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea008cb2-1d42-4bd8-9623-dbb911f09921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 14:44:20.866668: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-23 14:44:21.569127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 14:44:22.651174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d171f6-04a2-45ef-86e6-82e45d5a2a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 14:44:24.924667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-23 14:44:25.332721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-23 14:44:25.332759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe33cda-117d-4577-b511-2298e168177f",
   "metadata": {},
   "source": [
    "Brief dataset overview\n",
    "The dataset that I found contains all of the neccessary information about train journeys in the Netherlands, including station code, station name, train id, train type and so on. However from the spot I can say that the amount of features is too big - I would certainly have to get rid of some of them, for example feature called Stop:RDT-id is just autoincremented and would have little to no impact on models prediction.\n",
    "Moreover, though there's some NaN values that 'beg' to be cleaned, I certainly should put more effort into data cleaning than just typing df.dropna()\n",
    "\n",
    "You can see the dataset overview below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd4357-962e-496b-ab9a-daa8f17af914",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info = pd.read_csv('services-2022.csv')\n",
    "basic_info.head()\n",
    "#disruptions = pd.read_csv('disruptions-2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa467f5e-bf85-45e9-a619-6658fbda72ec",
   "metadata": {},
   "source": [
    "First steps:\n",
    "\n",
    "a) Rename columns in the dataset - currently the column's names in the dataset are too long and too complicated, which makes the data preprocessing part much harder, especially when dealing with specific columns names. So I've renamed the columns, making them more clear and concise\n",
    "\n",
    "b) Data cleaning. Dropping features\n",
    "Elaboration on dropping features:\n",
    "Train id, Stop id: this features just indicate the id of the entry, not pointing to any potential correlations\n",
    "Maximum delay, Arrival cancelled, Completely cancelled, Partly cancelled: Though this features could be of use, they do not follow the requirements - the user can't possibly know if train arrival was cancelled, and though it is possible to make the model compute this features by putting them into y_test, this would only make everything more complicated, so it is much easier just to remove this 4 features from the dataset.\n",
    "\n",
    "c) Data cleaning. Dealing with NaN values. \n",
    "After performing a short analysis, I've decided to deal with null values in this way:\n",
    "For NaN values in y_test labels: Drop the entire entry\n",
    "For NaN values in y_train where type is float or int: Change value from NaN to 0\n",
    "For any NaN value which is of type \"string\" or \"date\": Drop the entire entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fc564-33da-4d81-a5e7-942894c6d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = basic_info.columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8788298a-1a04-4262-91a1-8969dd1189ac",
   "metadata": {},
   "source": [
    "nan_mask = basic_info['Arrival delay'].isna()\n",
    "\n",
    "# Filter the DataFrame to keep only rows where 'Departure delay' is NaN\n",
    "filtered_df = basic_info[nan_mask]\n",
    "\n",
    "# Sort the DataFrame based on the 'Departure delay' column\n",
    "sorted_df = filtered_df.sort_values(by='Departure delay')\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "sorted_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e8888-5794-4b0b-aff8-721d4e2d4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def data_cleaning_main_dataset(dataset):\n",
    "        dataset.drop(\"Partly cancelled\", inplace = True, axis = 1)\n",
    "        dataset.drop(\"Maximum delay\", inplace = True, axis = 1)\n",
    "        dataset.drop(\"Stop id\", inplace = True, axis = 1)\n",
    "        dataset.drop(\"Train id\", inplace = True, axis = 1)\n",
    "        dataset.drop(\"Arrival cancelled\", inplace = True, axis = 1)\n",
    "        dataset.drop(\"Completely cancelled\", inplace = True, axis = 1)\n",
    "        dataset = dataset.dropna(axis=0, subset=['Departure delay'])\n",
    "        dataset = dataset.dropna(axis=0, subset=['Departure cancelled'])\n",
    "        dataset = dataset.dropna(axis=0, subset=['Departure time'])\n",
    "        dataset['Arrival delay'] = dataset['Arrival delay'].fillna(0)\n",
    "        return dataset\n",
    "\n",
    "    def rename_columns_main_dataset(dataset):\n",
    "        new_columns = {\n",
    "        'Service:RDT-ID': 'Train id',\n",
    "        'Service:Date': 'Date',\n",
    "        'Service:Type': 'Train type',\n",
    "        'Service:Company': 'Railroad company',\n",
    "        'Service:Train number': 'Train number',\n",
    "        'Service:Completely cancelled': 'Completely cancelled',\n",
    "        'Service:Partly cancelled': 'Partly cancelled',\n",
    "        'Service:Maximum delay': 'Maximum delay',\n",
    "        'Stop:RDT-ID': 'Stop id',\n",
    "        'Stop:Station code': 'Station code',\n",
    "        'Stop:Station name': 'Station name',\n",
    "        'Stop:Arrival time': 'Arrival time',\n",
    "        'Stop:Arrival delay': 'Arrival delay',\n",
    "        'Stop:Arrival cancelled': 'Arrival cancelled',\n",
    "        'Stop:Departure time': 'Departure time',\n",
    "        'Stop:Departure delay': 'Departure delay',\n",
    "        'Stop:Departure cancelled': 'Departure cancelled'\n",
    "    }\n",
    "\n",
    "        # Rename the columns\n",
    "        dataset = dataset.rename(columns=new_columns)\n",
    "        return dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88acf5-719e-48f5-86f1-d43503a307f5",
   "metadata": {},
   "source": [
    "Next step: Enriching the dataset with new features\n",
    "\n",
    "I've decided to add two more features that would be obtained from \"Date\" column. This two features would be of boolean type and would be named \"Is weekend\" and \"Is holiday\".\n",
    "The reason for adding this features is to extract as much information from the user's input as possible, as the information received from the user is qiute scarce - only 5 features. Moreover, it is important to add that dates contain more information than just plain numbers - for example according to open-source statistics, the total number of daily disruptions on weekends is almost 20% lower then on weekdays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c1ee6-eca2-4e2f-a42d-3d17feec54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_new_weekday_feature(dataset):\n",
    "    new_weekday_feature = []\n",
    "    for date_row in dataset['Date']:\n",
    "        convert_to_date = datetime.strptime(date_row, '%Y-%m-%d')\n",
    "        day_number = convert_to_date.weekday()\n",
    "        if (day_number >= 5):\n",
    "            new_weekday_feature.append(True)\n",
    "        else:\n",
    "            new_weekday_feature.append(False)\n",
    "    dataset['Is_weekend'] = new_weekday_feature\n",
    "    return dataset\n",
    "    \n",
    "def extract_holidays(dataset):\n",
    "    list_with_holidays = [\n",
    "    '2022-01-01',   # New Year's Day\n",
    "    '2022-04-15',   # Good Friday\n",
    "    '2022-04-17',   # Easter Sunday\n",
    "    '2022-04-18',   # Easter Monday\n",
    "    '2022-04-27',   # King's Day\n",
    "    '2022-05-04',   # Remembrance Day\n",
    "    '2022-05-05',   # Liberation Day\n",
    "    '2022-05-26',   # Ascension Day\n",
    "    '2022-06-05',   # Whit Sunday\n",
    "    '2022-06-06',   # Whit Monday\n",
    "    '2022-12-25',   # Christmas Day\n",
    "    '2022-12-26',   # Second Christmas Day\n",
    "    ]\n",
    "\n",
    "    # Convert the 'Date' column to strings in the same format\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date']).dt.strftime('%Y-%m-%d')\n",
    "    # Create the 'Is_holiday' column based on the 'Date' column\n",
    "    dataset['Is_holiday'] = dataset['Date'].isin(list_with_holidays)\n",
    "    # Create the 'Is_holiday' column based on the 'Date' column\n",
    "    return dataset\n",
    "    \n",
    "def data_preprocessing_main_dataset(dataset):\n",
    "    dataset = add_new_weekday_feature(dataset)\n",
    "    dataset = extract_holidays(dataset)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49065f-0097-40cd-abd6-164a492bc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disruptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44f248-5410-4724-861d-f3858da42395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_disruptions = disruptions[disruptions['duration_minutes'] > 3000]\n",
    "#large_disruptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b32003-4ca1-4c23-ad6d-814e508c90ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58df0d-0e6b-48a6-8ddf-c0b413b42b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dataset = basic_info[basic_info['Stop:Arrival cancelled'] == True]\n",
    "sorted_dataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddda08-d130-4039-8446-be695769d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows before cleaning (main dataset): \" + str(len(basic_info))) \n",
    "print(\"Total rows before cleaning (disruptions): \" + str(len(basic_info))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c248983-8e05-4fee-b6d6-e1fcf024f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info = rename_columns_main_dataset(basic_info)\n",
    "basic_info = data_cleaning_main_dataset(basic_info)\n",
    "basic_info = data_preprocessing_main_dataset(basic_info)\n",
    "basic_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec37b0-4758-4964-a38e-e5467b5fe0d5",
   "metadata": {},
   "source": [
    "Advanced data cleaning: Rare trains and small companies.\n",
    "After observing the amount and distribution of unique values in \"Train type\" and \"Railroad company\" columns I've found out that there's a lot of small values in both features, that take around 0.01-0.05% of the dataset (see bar charts below for more information)\n",
    "I've removed all entries from 'Train type' and 'Railroad company' columns where the distribution of the value within the feature is <1%. I believe that this would confuse the model much less, making the predictions more stable and precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212cdc7-3ae7-476f-b975-567ad1a6a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(label):\n",
    "    grouped_data = basic_info.groupby(label).size()\n",
    "    \n",
    "    # Plot the bar chart\n",
    "    grouped_data.plot(kind='bar', color='navajowhite', edgecolor='black')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of ' + label)\n",
    "    \n",
    "    # Add percentages on top of each bar\n",
    "    total_count = len(basic_info)  # Total number of entries in the DataFrame\n",
    "    for i, value in enumerate(grouped_data):\n",
    "        percentage = (value / total_count) * 100\n",
    "        plt.text(i, value + 0.1, f'{percentage:.2f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075cbc2-d536-4403-bd14-cd149291a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows after cleaning: \" + str(len(basic_info))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a04a8-38c0-42ce-a7c9-478ce0763627",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart('Railroad company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b2875-3afd-4bae-91d7-55c2091e0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart('Train type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f2e6f-9fae-41df-8c6a-5bb1b3bcd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_info.groupby('Train type').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b58a8-5da9-4204-9fc3-629aa3c8468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_info[\"Railroad company\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1422b13-7b24-4161-9c46-321b35a9f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_remove = ['NS Int', 'Eurobahn', 'Breng', 'DB', 'EB', 'NMBS', 'VIAS', 'ZLSM', 'keo', '.',\n",
    "                   'ABRN', 'Keolis', 'NSI', 'Railexpert', 'TCS', 'connexxi', 'db', 'nmbs', 'noord']\n",
    "basic_info['Railroad company'] = basic_info['Railroad company'].replace(values_to_remove, pd.NA)\n",
    "\n",
    "# Drop rows with NaN values in 'Railroad company' column\n",
    "basic_info = basic_info.dropna(subset=['Railroad company'])\n",
    "basic_info['Railroad company'] = basic_info['Railroad company'].replace('ns', 'NS')\n",
    "basic_info['Railroad company'] = basic_info['Railroad company'].replace('BN', 'Blauwnet')\n",
    "basic_info['Railroad company'] = basic_info['Railroad company'].replace('Rnet', 'R-net')\n",
    "plot_bar_chart('Railroad company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b3368-42d6-4451-a624-13e5e1c0064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_remove = ['Bus', 'Eurostar', 'Extra trein', 'ICE International', 'Int. Trein', 'Nachttrein', 'Nightjet',\n",
    "                    'Snelbus i.p.v. trein', 'Stoomtrein', 'Thalys', 'Speciale Trein', 'Stopbus i.p.v. trein', \n",
    "                    'Metro i.p.v. trein', 'Dinnner Train', 'Alpen Express']\n",
    "basic_info['Train type'] = basic_info['Train type'].replace(values_to_remove, pd.NA)\n",
    "# Drop rows with NaN values in 'Railroad company' column\n",
    "basic_info = basic_info.dropna(subset=['Train type'])\n",
    "plot_bar_chart('Train type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ebabf-caa0-41bf-9e0c-c146c0b1aae1",
   "metadata": {},
   "source": [
    "Below: Plotting features that still have null values in them. Removing null values from those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5482ca-e394-46e4-b450-5ff7d0c3ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nas(df: pd.DataFrame):\n",
    "    if df.isnull().sum().sum() != 0:\n",
    "        na_df = (df.isnull().sum() / len(df)) * 100      \n",
    "        na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
    "        missing_data = pd.DataFrame({'Missing Ratio %' :na_df})\n",
    "        missing_data.plot(kind = \"barh\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No NAs found')\n",
    "plot_nas(basic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31cc2a-a9cb-4b3d-a682-fd764ae21fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info = basic_info.dropna(axis=0, subset=['Station code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0ad66-6734-408e-bc53-c70dbbc6cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows after cleaning: \" + str(len(basic_info))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5fc17-0338-4bd4-9508-01358ac36e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c6265-c9de-4a69-93ad-c3e7a472773f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551c1ab-f206-4870-94cd-bc20c5a6ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('preprocessed_data/main_preprocessed_dataset.csv')  \n",
    "basic_info.to_csv(filepath)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
